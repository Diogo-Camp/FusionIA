#roda ollama usando todas as threads -2. ollamanocuda=0 forca o uso da gpu, ollama_max_batch, controla a quantidade de tokens processados de uma vez (cuidado com o uso de RAM).
OLLAMA_NUM_THREAD=$(( $(nproc) - 2 )) OLLAMA_NO_CUDA=0 OLLAMA_MAX_BATCH=4096 ollama run mistral:instruct


 Linha m√°gica pra rodar Ollama com tudo no m√°ximo
Voc√™ pode for√ßar o uso m√°ximo da GPU e boa parte da CPU assim:

Se estiver fora do Docker:
bash
Copy
Edit
OLLAMA_NUM_THREAD=$(nproc) ollama run mistral:instruct
Isso diz ao Ollama pra usar todos os n√∫cleos dispon√≠veis.
(Obs: se sua CPU tem Hyper-Threading, ele vai usar todos os threads l√≥gicos)

Se quiser mais controle:

bash
Copy
Edit
OLLAMA_NUM_THREAD=16 OLLAMA_MAX_BATCH=4096 ollama run mistral:instruct
MAX_BATCH controla a quantidade de tokens processados de uma vez (cuidado com o uso de RAM).

Se estiver usando Docker:
bash
Copy
Edit
docker run --gpus all -e OLLAMA_NUM_THREAD=$(nproc) -p 11434:11434 ollama/ollama
O flag --gpus all ativa o uso de GPU (requer driver + CUDA setado corretamente).

üí° Extras pra otimizar + acelerar:
Use modelos quantizados Q4_K_M ou Q5_1
‚Üí √ìtimo equil√≠brio entre velocidade e qualidade

Verifique se a GPU t√° em uso:

bash
Copy
Edit
watch nvidia-smi
Melhore os tempos com cache pr√©-carregado:

bash
Copy
Edit
ollama pull mistral:instruct
Evite reprocessar prompts longos: use contexto encurtado ‚Üí Quanto menor o prompt, mais r√°pido gera




‚úÖ Vers√£o para terminal (fora do Docker):
bash
Copy
Edit
OLLAMA_NUM_THREAD=$(nproc) OLLAMA_MAX_BATCH=4096 ollama run mistral:instruct
üîç Explicando:
OLLAMA_NUM_THREAD=$(nproc) ‚Üí Usa todos os n√∫cleos da CPU dispon√≠veis

OLLAMA_MAX_BATCH=4096 ‚Üí Aumenta o n√∫mero de tokens processados por lote, usando mais RAM e acelerando o throughput

mistral:instruct ‚Üí Pode trocar por outro modelo que voc√™ tiver, como llama2:7b-chat, etc.
OLLAMA_NUM_THREAD=$(( $(nproc) - 2 )) OLLAMA_NO_CUDA=0 OLLAMA_MAX_BATCH=4096 ollama run mistral:instruct
mistral.Q4_K_M.mistral.Q4_K_M.




‚úÖ Se estiver rodando via Docker:
bash
Copy
Edit
docker run --gpus all -e OLLAMA_NUM_THREAD=$(nproc) -e OLLAMA_MAX_BATCH=409